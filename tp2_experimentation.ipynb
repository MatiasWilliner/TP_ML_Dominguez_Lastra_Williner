{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d91ed06",
   "metadata": {},
   "source": [
    "# Trabajo Práctico 2: Entrenamiento y evaluación de modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85887360",
   "metadata": {},
   "source": [
    "El caso de uso que se busca cubrir hace referencia a detectar si una jugada termina en home run o no, cuando la bola ya fue bateada. Solo se va a poder usar dicho modelo para predecir el resultado de una jugada si se tienen los datos del lanzamiento y el bateo. En consecuencia, podemos decir que su principal uso puede relacionarse con el estudio y análisis de las mejores formas para realizar un home run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5622308a",
   "metadata": {},
   "source": [
    "A partir del dataset elegido y ya preprocesado con la lógica definida en el TP1, el grupo debe realizar el entrenamiento y evaluación de al menos 3 algoritmos de machine learning.\n",
    "\n",
    "   Se debe elegir y definir una métrica de performance a utilizar para evaluar los modelos. Fundamentar la elección de la métrica.\n",
    "   Se debe aplicar alguna técnica de feature engineering para mejorar los datos de entrada a los modelos, y mostrar la comparativa de los resultados obtenidos en cada caso. Si no es posible o útil, fundamentar el motivo por el cual no se realizará.\n",
    "   Por cada modelo, se debe entrenarlo y realizar una exploración de hiper-parámetros mediante una búsqueda en grilla. Evaluar el comportamiento de cada modelo con los hiper-parámetros que mejores resultados ofrecen. En caso de ser posible, aporte conclusiones respecto a dicha comparación.\n",
    "   Realizar experimentos que utilicen como datos de entrada representaciones intermedias de los datos (generadas por técnicas de reducción de dimensiones como PCA). Compare los resultados obtenidos contra los casos previos, interprete y proponga conclusiones.\n",
    "   Se deben utilizar técnicas que garanticen que los modelos no están sobreentrenando sin que nos demos cuenta.\n",
    "   Determinar el valor final de la métrica que podría ser informado al cliente, utilizando técnicas que permitan obtener un valor lo más realista posible. Fundamentar y considerar no solo el rendimiento del modelo en su elección, sino también cuestiones como interpretabilidad, tiempos de entrenamiento, etc.\n",
    "   Para el método propuesto como definitivo, y para distintos pares de variables, genere diagramas de dispersión donde se visualicen los aciertos y errores del mismo. Discuta si existen patrones o conocimiento que se pueda obtener a partir de dichos errores. En caso de ser posible, evalúe la importancia que asigna el método a las variables de entrada y genere conclusiones al respecto.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda09d4d",
   "metadata": {},
   "source": [
    "## Tratamiento de datos aplicando en el TPN°1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3e2fc07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos las dependencias necesarias.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from sklearn.pipeline import Pipeline\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import SimpleImputer, IterativeImputer\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, OneHotEncoder\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40f4c99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d187f741",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Arreglamos el dataset según lo establecido en el TP1\n",
    "\n",
    "# Importamos train.csv y park_dimensions.csv, los unimos utilizando la variable \"park\"\n",
    "entrenamiento = pd.read_csv('./train.csv')\n",
    "estadio = pd.read_csv('./park_dimensions.csv')\n",
    "completa=entrenamiento.merge(estadio, on=\"park\", how=\"left\")\n",
    "\n",
    "# Desechamos las variables no utilizadas\n",
    "completa = completa.drop(['park','bip_id','batter_id','pitcher_id'],axis=1)\n",
    "\n",
    "# Asignamos nuevos nombres a las columnas\n",
    "renamed_columns = {'NAME': 'name', 'Cover': 'cover', 'LF_Dim': 'lf_dim', 'CF_Dim':'cf_dim',\n",
    "                   'RF_Dim': 'rf_dim', 'LF_W': 'lf_w', 'CF_W': 'cf_w', 'RF_W': 'rf_w'\n",
    "                  }\n",
    "completa.rename(columns=renamed_columns, inplace=True)\n",
    "\n",
    "# Convertir columna \"game_date\" de tipo object/string, a datetime\n",
    "completa['game_date'] = pd.to_datetime(completa['game_date'])\n",
    "\n",
    "# Eliminar datos filas con datos nulos en bb_type\n",
    "completa = completa[~completa.bb_type.isnull()]\n",
    "#Delimitación de conjuntos\n",
    "completa.isnull().sum()\n",
    "\n",
    "\n",
    "east = ['TB','BAL','BOS','TOR','NYY','ATL','MIA','PHI','NYM','WSH']\n",
    "central = ['MIN','CLE','DET','CWS','KC','PIT','MIL','CHC', 'CIN', 'STL']\n",
    "west = ['TEX','LAA','HOU','SEA','OAK','LAD','SD','SF','COL','ARI']\n",
    "\n",
    "def division_h(row):\n",
    "    if east.count(row['home_team']) > 0:\n",
    "        return 'east'\n",
    "    else:\n",
    "        if central.count(row['home_team']) > 0:\n",
    "            return 'central'\n",
    "        else:\n",
    "            return 'west'\n",
    "        \n",
    "def division_a(row):\n",
    "    if east.count(row['away_team']) > 0:\n",
    "        return 'east'\n",
    "    else:\n",
    "        if central.count(row['away_team']) > 0:\n",
    "            return 'central'\n",
    "        else:\n",
    "            return 'west'\n",
    "        \n",
    "division_home = completa.apply(division_h, axis=1)\n",
    "division_away = completa.apply(division_a, axis=1)\n",
    "completa[\"division_home\"] = division_home\n",
    "completa[\"division_away\"] = division_away\n",
    "completa.sample(10)\n",
    "\n",
    "columnas_string=['game_date','home_team','away_team','batter_team','batter_name','pitcher_name','name']\n",
    "completa = completa.drop(columnas_string,axis=1)\n",
    "#completa[[\"division_away\",'away_team']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df38961",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "77439fd1",
   "metadata": {},
   "source": [
    "## Selección de métrica\n",
    "\n",
    "Se debe elegir y definir una métrica de performance a utilizar para evaluar los modelos. Fundamentar la elección de la métrica."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0e8e02",
   "metadata": {},
   "source": [
    "Con respecto a las métricas, podemos decir que vamos a utilizar recall y precisión. Esto se debe a que los datos están muy desbalanceados, y en consecuencia accuracy no sería la mejor opción porque presentaría problemas.\n",
    "\n",
    " - Recall nos va a permitir ver de todos los Foo que había, cuántos encontramos.\n",
    "\n",
    " - Precisión que nos va a permitir ver de lo que clasificamos como Foo, qué porcentaje era realmente Foo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfadfb89",
   "metadata": {},
   "source": [
    "## Aplicaciones de featuring engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05bc9cb7",
   "metadata": {},
   "source": [
    "En lo referente a feature enginering hay que hay que mencionar que vamos a aplicar la técnica de QuantileTransformer para las variables de entrada que tienen valores extremos según lo observado en el TP1. De esta forma se logra reducir el impacto de los valores atípicos. También vamos a transformar algunas variables para transformar la información de las mismas expresada de otra forma. \n",
    "Aplicamos técnicas de preprocesado para mejorar la representación de los datos como OneHotEncoder y StandardImputer, y eliminamos los valores nulos utilizando SimpleImputer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252a5b92",
   "metadata": {},
   "source": [
    "### División del dataset.\n",
    "\n",
    "El dataset se va a dividir en tres grupos:\n",
    " - Train (60%)\n",
    " - Test (20%)\n",
    " - Validation (20%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d26736c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((27742, 24), (9248, 24), (9248, 24))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#División de datasets en \"train\", \"test\" y \"validation\"\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, not_train = train_test_split(completa, test_size=0.4, random_state=42)\n",
    "validation, test = train_test_split(not_train, test_size=0.5, random_state=42)\n",
    "\n",
    "train.shape, validation.shape, test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db2b4ba",
   "metadata": {},
   "source": [
    "### Aplicación de DataFrameMapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "bb18c1bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-7 {color: black;background-color: white;}#sk-container-id-7 pre{padding: 0;}#sk-container-id-7 div.sk-toggleable {background-color: white;}#sk-container-id-7 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-7 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-7 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-7 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-7 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-7 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-7 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-7 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-7 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-7 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-7 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-7 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-7 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-7 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-7 div.sk-item {position: relative;z-index: 1;}#sk-container-id-7 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-7 div.sk-item::before, #sk-container-id-7 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-7 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-7 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-7 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-7 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-7 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-7 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-7 div.sk-label-container {text-align: center;}#sk-container-id-7 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-7 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-7\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DataFrameMapper(drop_cols=[],\n",
       "                features=[([&#x27;pitch_mph&#x27;], [StandardScaler()]),\n",
       "                          ([&#x27;launch_speed&#x27;], [StandardScaler()]),\n",
       "                          ([&#x27;launch_angle&#x27;], [StandardScaler()]),\n",
       "                          ([&#x27;is_home_run&#x27;], None),\n",
       "                          ([&#x27;lf_dim&#x27;], [MinMaxScaler()]),\n",
       "                          ([&#x27;cf_dim&#x27;], [MinMaxScaler()]),\n",
       "                          ([&#x27;rf_dim&#x27;], [MinMaxScaler()]),\n",
       "                          ([&#x27;lf_w&#x27;], [MinMaxScaler()]),\n",
       "                          ([&#x27;cf_w&#x27;], [MinMaxScaler()]),\n",
       "                          ([&#x27;rf_w&#x27;], [MinMaxScaler()]),\n",
       "                          ([&#x27;division_home&#x27;], [OneHotEncoder()]),\n",
       "                          ([&#x27;division_away&#x27;], [OneHotEncoder()])])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" checked><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DataFrameMapper</label><div class=\"sk-toggleable__content\"><pre>DataFrameMapper(drop_cols=[],\n",
       "                features=[([&#x27;pitch_mph&#x27;], [StandardScaler()]),\n",
       "                          ([&#x27;launch_speed&#x27;], [StandardScaler()]),\n",
       "                          ([&#x27;launch_angle&#x27;], [StandardScaler()]),\n",
       "                          ([&#x27;is_home_run&#x27;], None),\n",
       "                          ([&#x27;lf_dim&#x27;], [MinMaxScaler()]),\n",
       "                          ([&#x27;cf_dim&#x27;], [MinMaxScaler()]),\n",
       "                          ([&#x27;rf_dim&#x27;], [MinMaxScaler()]),\n",
       "                          ([&#x27;lf_w&#x27;], [MinMaxScaler()]),\n",
       "                          ([&#x27;cf_w&#x27;], [MinMaxScaler()]),\n",
       "                          ([&#x27;rf_w&#x27;], [MinMaxScaler()]),\n",
       "                          ([&#x27;division_home&#x27;], [OneHotEncoder()]),\n",
       "                          ([&#x27;division_away&#x27;], [OneHotEncoder()])])</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DataFrameMapper(drop_cols=[],\n",
       "                features=[(['pitch_mph'], [StandardScaler()]),\n",
       "                          (['launch_speed'], [StandardScaler()]),\n",
       "                          (['launch_angle'], [StandardScaler()]),\n",
       "                          (['is_home_run'], None),\n",
       "                          (['lf_dim'], [MinMaxScaler()]),\n",
       "                          (['cf_dim'], [MinMaxScaler()]),\n",
       "                          (['rf_dim'], [MinMaxScaler()]),\n",
       "                          (['lf_w'], [MinMaxScaler()]),\n",
       "                          (['cf_w'], [MinMaxScaler()]),\n",
       "                          (['rf_w'], [MinMaxScaler()]),\n",
       "                          (['division_home'], [OneHotEncoder()]),\n",
       "                          (['division_away'], [OneHotEncoder()])])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Definimos el mapper. Recibe una lista de (columna/s, transformers)\n",
    "\n",
    "mapper = DataFrameMapper([ \n",
    "    #(['is_batter_lefty'], None),\n",
    "    #(['is_pitcher_lefty'], None),\n",
    "    #(['bb_type'], [OneHotEncoder()]),\n",
    "    #(['bearing'], [OneHotEncoder()]),\n",
    "    #(['pitch_name'], [OneHotEncoder()]),\n",
    "    #(['inning'], [StandardScaler()]),\n",
    "    #(['outs_when_up'], None),\n",
    "    #(['balls'], [StandardScaler()]),\n",
    "    #(['strikes'], [MinMaxScaler()]),\n",
    "    #(['plate_x'], [MinMaxScaler()]),\n",
    "    #(['plate_z'], [MinMaxScaler()]),\n",
    "    (['pitch_mph'], [StandardScaler()]),\n",
    "    (['launch_speed'], [StandardScaler()]),\n",
    "    (['launch_angle'], [StandardScaler()]),\n",
    "    (['is_home_run'], None),\n",
    "    #(['cover'], [OneHotEncoder()]),\n",
    "    (['lf_dim'], [MinMaxScaler()]),\n",
    "    (['cf_dim'], [MinMaxScaler()]),\n",
    "    (['rf_dim'], [MinMaxScaler()]),\n",
    "    (['lf_w'], [MinMaxScaler()]),\n",
    "    (['cf_w'], [MinMaxScaler()]),\n",
    "    (['rf_w'], [MinMaxScaler()]),\n",
    "    ([\"division_home\"], [OneHotEncoder()]),\n",
    "    ([\"division_away\"], [OneHotEncoder()]),\n",
    "])\n",
    "\n",
    "mapper.fit(train)\n",
    "\n",
    "#completa_processed = mapper.transform(completa)\n",
    "#completa_processed = pd.DataFrame(completa_processed, columns=mapper.transformed_names_)\n",
    "#completa_processed\n",
    "#mapper.transformed_names_\n",
    "#completa_processed.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e80a88c",
   "metadata": {},
   "source": [
    "### Eliminar nulos y aplicar Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547e9dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combinación de lo anterior\n",
    "\n",
    "pipe1 = Pipeline([\n",
    "    ('mapper', mapper),\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "])\n",
    "# Lo entrenamos con train\n",
    "pipe1.fit(completa)\n",
    "completa_com = pipe1.transform(completa)\n",
    "completa_com=pd.DataFrame(completa_com, columns=mapper.transformed_names_)\n",
    "completa_com.isnull().sum()\n",
    "#train_b\n",
    "#mapper.transformed_names_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2ed3f4",
   "metadata": {},
   "source": [
    "### Aplicación de QuantileTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1b4bbc",
   "metadata": {},
   "source": [
    "Según lo visto en el TP1, consideramos necesario aplicar dicha técnica a los variables \"plate_x\", \"plate_z\", \"pitch_mph\", \"launch_speed\" y \"launch_angle\". Esto se debe a que contienen valores extremos que pueden traer problemas para la predicción."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c58d0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_histogram_transformation(columna):\n",
    "\n",
    "    fig, axis = plt.subplots(1, 2, figsize=(12, 4), sharey=True)\n",
    "\n",
    "    x = columna\n",
    "    x.hist(ax=axis[0], bins=20)\n",
    "    axis[0].set_title('Distribución original')\n",
    "    \n",
    "    tr = QuantileTransformer(n_quantiles=50, output_distribution='normal')\n",
    "    tr.fit(x.to_frame())\n",
    "\n",
    "    x2 = tr.transform(x.to_frame())\n",
    "    pd.Series(x2.flatten()).hist(ax=axis[1], bins=20)\n",
    "    axis[1].set_title('Distribución normal')\n",
    "    return x2\n",
    "\n",
    "#plot_histogram_transformation()\n",
    "#completa['pitch_mph']= plot_histogram_transformation(completa.plate_x)\n",
    "#completa['pitch_mph']\n",
    "#completa['pitch_mph']= plot_histogram_transformation(completa.plate_z)\n",
    "#completa['pitch_mph']\n",
    "completa['pitch_mph']= plot_histogram_transformation(completa.pitch_mph)\n",
    "#completa['pitch_mph']\n",
    "#completa['launch_speed']= plot_histogram_transformation(completa.launch_speed)\n",
    "#completa['pitch_mph']\n",
    "#completa['pitch_mph']= plot_histogram_transformation(completa.launch_angle)\n",
    "#completa['pitch_mph']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4960022",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43c92e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c33ada",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd0ab1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a527916e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52eb69e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3530be60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de7e747",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "31c4bdff",
   "metadata": {},
   "source": [
    "## Modelos a utilizar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119178d6",
   "metadata": {},
   "source": [
    "Los modelos que vamos a utilizar van a ser:\n",
    " - LogisticRegression\n",
    " - KNeighborsClassifier\n",
    " - GridSearchCV\n",
    " - RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8fedfefd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      8759\n",
      "           1       1.00      1.00      1.00       489\n",
      "\n",
      "    accuracy                           1.00      9248\n",
      "   macro avg       1.00      1.00      1.00      9248\n",
      "weighted avg       1.00      1.00      1.00      9248\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "pipel = Pipeline([\n",
    "    ('mapper', mapper),\n",
    "    ('imputer', IterativeImputer(random_state=42)),\n",
    "    ('classifier', LogisticRegression(random_state=42)),\n",
    "])\n",
    "\n",
    "pipel.fit(train, train.is_home_run)\n",
    "\n",
    "y_pred = pipel.predict(test)\n",
    "y_pred\n",
    "print(metrics.classification_report(test.is_home_run, y_pred))\n",
    "metrics.precision_score(test.is_home_run, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ddd17f51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99      8795\n",
      "           1       1.00      0.78      0.87       453\n",
      "\n",
      "    accuracy                           0.99      9248\n",
      "   macro avg       0.99      0.89      0.93      9248\n",
      "weighted avg       0.99      0.99      0.99      9248\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7770419426048565"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K=200\n",
    "knn_model = Pipeline([\n",
    "    ('mapper', mapper),\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('classifier', KNeighborsClassifier(n_neighbors=K)),\n",
    "])\n",
    "\n",
    "knn_model.fit(train, train.is_home_run)\n",
    "y_pred = knn_model.predict(validation)\n",
    "y_pred\n",
    "print(metrics.classification_report(validation.is_home_run, y_pred))\n",
    "\n",
    "\n",
    "metrics.recall_score(validation.is_home_run, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "8cdf59a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00      8795\n",
      "           1       1.00      0.89      0.94       453\n",
      "\n",
      "    accuracy                           0.99      9248\n",
      "   macro avg       1.00      0.95      0.97      9248\n",
      "weighted avg       0.99      0.99      0.99      9248\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8940397350993378"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "forest_model_limits= RandomForestClassifier(n_estimators=100, max_depth=3, max_features=3, random_state=42)\n",
    "# n_estimators? max_depth=3?, max_features=2?\n",
    "\n",
    "rf_model = Pipeline([\n",
    "    ('mapper', mapper),\n",
    "    ('imputer', IterativeImputer(random_state=42)),\n",
    "    ('classifier', forest_model_limits),\n",
    "])\n",
    "\n",
    "rf_model.fit(train, train.is_home_run)\n",
    "\n",
    "y_pred = rf_model.predict(validation)\n",
    "y_pred\n",
    "print(metrics.classification_report(validation.is_home_run, y_pred))\n",
    "\n",
    "\n",
    "metrics.recall_score(validation.is_home_run, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4d8579f3",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Cannot clone object. You should provide an instance of scikit-learn estimator instead of a class.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[68], line 14\u001b[0m\n\u001b[1;32m      6\u001b[0m clf \u001b[38;5;241m=\u001b[39m GridSearchCV(KNeighborsClassifier, parameters, refit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      8\u001b[0m gs_pipe \u001b[38;5;241m=\u001b[39m Pipeline([\n\u001b[1;32m      9\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmapper\u001b[39m\u001b[38;5;124m'\u001b[39m, mapper),\n\u001b[1;32m     10\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimputer\u001b[39m\u001b[38;5;124m'\u001b[39m, SimpleImputer(strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m)),\n\u001b[1;32m     11\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclassifier\u001b[39m\u001b[38;5;124m'\u001b[39m, clf),\n\u001b[1;32m     12\u001b[0m ])\n\u001b[0;32m---> 14\u001b[0m \u001b[43mgs_pipe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_home_run\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m gs_pipe\u001b[38;5;241m.\u001b[39mpredict(validation)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m#clf.best_score_, clf.best_params_\u001b[39;00m\n",
      "File \u001b[0;32m~/jupyter/jupyter/lib/python3.10/site-packages/sklearn/pipeline.py:405\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    403\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    404\u001b[0m         fit_params_last_step \u001b[38;5;241m=\u001b[39m fit_params_steps[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]]\n\u001b[0;32m--> 405\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_final_estimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params_last_step\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/jupyter/jupyter/lib/python3.10/site-packages/sklearn/model_selection/_search.py:788\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    785\u001b[0m cv_orig \u001b[38;5;241m=\u001b[39m check_cv(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv, y, classifier\u001b[38;5;241m=\u001b[39mis_classifier(estimator))\n\u001b[1;32m    786\u001b[0m n_splits \u001b[38;5;241m=\u001b[39m cv_orig\u001b[38;5;241m.\u001b[39mget_n_splits(X, y, groups)\n\u001b[0;32m--> 788\u001b[0m base_estimator \u001b[38;5;241m=\u001b[39m \u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    790\u001b[0m parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs, pre_dispatch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpre_dispatch)\n\u001b[1;32m    792\u001b[0m fit_and_score_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[1;32m    793\u001b[0m     scorer\u001b[38;5;241m=\u001b[39mscorers,\n\u001b[1;32m    794\u001b[0m     fit_params\u001b[38;5;241m=\u001b[39mfit_params,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    800\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[1;32m    801\u001b[0m )\n",
      "File \u001b[0;32m~/jupyter/jupyter/lib/python3.10/site-packages/sklearn/base.py:73\u001b[0m, in \u001b[0;36mclone\u001b[0;34m(estimator, safe)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(estimator, \u001b[38;5;28mtype\u001b[39m):\n\u001b[0;32m---> 73\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m     74\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot clone object. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     75\u001b[0m             \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou should provide an instance of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     76\u001b[0m             \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscikit-learn estimator instead of a class.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     77\u001b[0m         )\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     79\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m     80\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot clone object \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m (type \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m): \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     81\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mit does not seem to be a scikit-learn \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     82\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mestimator as it does not implement a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     83\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mget_params\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m method.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mrepr\u001b[39m(estimator), \u001b[38;5;28mtype\u001b[39m(estimator))\n\u001b[1;32m     84\u001b[0m         )\n",
      "\u001b[0;31mTypeError\u001b[0m: Cannot clone object. You should provide an instance of scikit-learn estimator instead of a class."
     ]
    }
   ],
   "source": [
    "#Grid Search:\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameters = {'n_neighbors': [100, 2, 50]}\n",
    "\n",
    "clf = GridSearchCV(KNeighborsClassifier, parameters, refit=True, verbose=1)\n",
    "\n",
    "gs_pipe = Pipeline([\n",
    "    ('mapper', mapper),\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('classifier', clf),\n",
    "])\n",
    "\n",
    "gs_pipe.fit(train, train.is_home_run)\n",
    "gs_pipe.predict(validation)\n",
    "#clf.best_score_, clf.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6c3859",
   "metadata": {},
   "source": [
    "Por cada modelo, se debe entrenarlo y realizar una exploración de hiper-parámetros mediante una búsqueda en grilla. Evaluar el comportamiento de cada modelo con los hiper-parámetros que mejores resultados ofrecen. En caso de ser posible, aporte conclusiones respecto a dicha comparación."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcffd1fd",
   "metadata": {},
   "source": [
    "## Técnicas de reducción de la dimensionalidad "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53d0033",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9d95aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_pipe = make_pipeline(StandardScaler(), PCA())\n",
    "pca_pipe.fit(train_b)\n",
    "\n",
    "modelo_pca = pca_pipe.named_steps['pca']\n",
    "print(train_b.columns)\n",
    "\n",
    "print('----------------------------------------------------')\n",
    "print('Porcentaje de varianza explicada por cada componente')\n",
    "print('----------------------------------------------------')\n",
    "print(modelo_pca.explained_variance_ratio_)\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(6, 4))\n",
    "ax.bar(\n",
    "    x      = np.arange(modelo_pca.n_components_) + 1,\n",
    "    height = modelo_pca.explained_variance_ratio_\n",
    ")\n",
    "\n",
    "for x, y in zip(np.arange(len(train_b.columns)) + 1, modelo_pca.explained_variance_ratio_):\n",
    "    label = round(y, 2)\n",
    "    ax.annotate(\n",
    "        label,\n",
    "        (x,y),\n",
    "        textcoords=\"offset points\",\n",
    "        xytext=(0,10),\n",
    "        ha='center'\n",
    "    )\n",
    "\n",
    "ax.set_xticks(np.arange(modelo_pca.n_components_) + 1)\n",
    "ax.set_ylim(0, 1.1)\n",
    "ax.set_title('Porcentaje de varianza explicada por cada componente')\n",
    "ax.set_xlabel('Componente principal')\n",
    "ax.set_ylabel('Por. varianza explicada');\n",
    "#train_processed.head=X_cols\n",
    "\n",
    "#pca= PCA(n_components=4,random_state=42)\n",
    "#train= pca.fit_transform(train_b)\n",
    "#[:37]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b4cf45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
