{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d91ed06",
   "metadata": {},
   "source": [
    "# Trabajo Práctico 2: Entrenamiento y evaluación de modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85887360",
   "metadata": {},
   "source": [
    "El caso de uso que se busca cubrir hace referencia a detectar si una jugada termina en home run o no, cuando la bola ya fue bateada. Solo se va a poder usar dicho modelo para predecir el resultado de una jugada si se tienen los datos del lanzamiento y el bateo. En consecuencia, podemos decir que su principal uso puede relacionarse con el estudio y análisis de las mejores formas para realizar un home run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5622308a",
   "metadata": {},
   "source": [
    "A partir del dataset elegido y ya preprocesado con la lógica definida en el TP1, el grupo debe realizar el entrenamiento y evaluación de al menos 3 algoritmos de machine learning.\n",
    "\n",
    "   Se debe elegir y definir una métrica de performance a utilizar para evaluar los modelos. Fundamentar la elección de la métrica.\n",
    "   Se debe aplicar alguna técnica de feature engineering para mejorar los datos de entrada a los modelos, y mostrar la comparativa de los resultados obtenidos en cada caso. Si no es posible o útil, fundamentar el motivo por el cual no se realizará.\n",
    "   Por cada modelo, se debe entrenarlo y realizar una exploración de hiper-parámetros mediante una búsqueda en grilla. Evaluar el comportamiento de cada modelo con los hiper-parámetros que mejores resultados ofrecen. En caso de ser posible, aporte conclusiones respecto a dicha comparación.\n",
    "   Realizar experimentos que utilicen como datos de entrada representaciones intermedias de los datos (generadas por técnicas de reducción de dimensiones como PCA). Compare los resultados obtenidos contra los casos previos, interprete y proponga conclusiones.\n",
    "   Se deben utilizar técnicas que garanticen que los modelos no están sobreentrenando sin que nos demos cuenta.\n",
    "   Determinar el valor final de la métrica que podría ser informado al cliente, utilizando técnicas que permitan obtener un valor lo más realista posible. Fundamentar y considerar no solo el rendimiento del modelo en su elección, sino también cuestiones como interpretabilidad, tiempos de entrenamiento, etc.\n",
    "   Para el método propuesto como definitivo, y para distintos pares de variables, genere diagramas de dispersión donde se visualicen los aciertos y errores del mismo. Discuta si existen patrones o conocimiento que se pueda obtener a partir de dichos errores. En caso de ser posible, evalúe la importancia que asigna el método a las variables de entrada y genere conclusiones al respecto.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda09d4d",
   "metadata": {},
   "source": [
    "## Tratamiento de datos aplicados en el trabajo práctico N.º 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e2fc07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos las dependencias necesarias.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from sklearn.pipeline import Pipeline\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import SimpleImputer, IterativeImputer\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, OneHotEncoder\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d187f741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arreglamos el dataset según lo establecido en el TP1\n",
    "\n",
    "# Importamos train.csv y park_dimensions.csv, los unimos utilizando la variable \"park\"\n",
    "entrenamiento = pd.read_csv('./train.csv')\n",
    "estadio = pd.read_csv('./park_dimensions.csv')\n",
    "completa=entrenamiento.merge(estadio, on=\"park\", how=\"left\")\n",
    "\n",
    "# Desechamos las variables no utilizadas\n",
    "completa = completa.drop(['park','bip_id','batter_id','pitcher_id'],axis=1)\n",
    "\n",
    "# Asignamos nuevos nombres a las columnas\n",
    "renamed_columns = {'NAME': 'name', 'Cover': 'cover', 'LF_Dim': 'lf_dim', 'CF_Dim':'cf_dim',\n",
    "                   'RF_Dim': 'rf_dim', 'LF_W': 'lf_w', 'CF_W': 'cf_w', 'RF_W': 'rf_w'\n",
    "                  }\n",
    "completa.rename(columns=renamed_columns, inplace=True)\n",
    "\n",
    "# Convertir columna \"game_date\" de tipo object/string, a datetime\n",
    "completa['game_date'] = pd.to_datetime(completa['game_date'])\n",
    "\n",
    "# Eliminar datos filas con datos nulos en bb_type\n",
    "completa = completa[~completa.bb_type.isnull()]\n",
    "\n",
    "# Delimitación de conjuntos\n",
    "completa.isnull().sum()\n",
    "\n",
    "# Crear df a la que se le va aplicar feature engineering\n",
    "completa_fe = completa\n",
    "columnas_string=['game_date', 'batter_team','batter_name','pitcher_name','name']\n",
    "completa_fe = completa_fe.drop(columnas_string,axis=1)\n",
    "\n",
    "# Eliminar columnas cuyos valores sean cadenas\n",
    "columnas_string=['home_team','away_team', 'game_date', 'batter_team','batter_name','pitcher_name','name']\n",
    "completa = completa.drop(columnas_string,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252a5b92",
   "metadata": {},
   "source": [
    "## División del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26736c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividimos el dataset en train (60%), test (20%) y validation (20%)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, not_train = train_test_split(completa, test_size=0.4, random_state=42)\n",
    "validation, test = train_test_split(not_train, test_size=0.5, random_state=42)\n",
    "\n",
    "train.shape, validation.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb18c1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos el mapper. Recibe una lista de (columna/s, transformers)\n",
    "\n",
    "mapper = DataFrameMapper([ \n",
    "    #(['is_batter_lefty'], None),\n",
    "    #(['is_pitcher_lefty'], None),\n",
    "    #(['bb_type'], [OneHotEncoder()]),\n",
    "    #(['bearing'], [OneHotEncoder()]),\n",
    "    #(['pitch_name'], [OneHotEncoder()]),\n",
    "    #(['inning'], [StandardScaler()]),\n",
    "    #(['outs_when_up'], None),\n",
    "    #(['balls'], [StandardScaler()]),\n",
    "    #(['strikes'], [StandardScaler()]),\n",
    "    #(['plate_x'], [StandardScaler()]),\n",
    "    #(['plate_z'], [StandardScaler()]),\n",
    "    (['pitch_mph'], [StandardScaler()]),\n",
    "    (['launch_speed'], [StandardScaler()]),\n",
    "    (['launch_angle'], [StandardScaler()]),\n",
    "    (['is_home_run'], None),\n",
    "    #(['cover'], [OneHotEncoder()]),\n",
    "    (['lf_dim'], [StandardScaler()]),\n",
    "    (['cf_dim'], [StandardScaler()]),\n",
    "    (['rf_dim'], [StandardScaler()]),\n",
    "    (['lf_w'], [StandardScaler()]),\n",
    "    (['cf_w'], [StandardScaler()]),\n",
    "    (['rf_w'], [StandardScaler()]),\n",
    "])\n",
    "\n",
    "mapper.fit(train)\n",
    "\n",
    "#completa_processed = mapper.transform(completa)\n",
    "#completa_processed = pd.DataFrame(completa_processed, columns=mapper.transformed_names_)\n",
    "#completa_processed\n",
    "#mapper.transformed_names_\n",
    "#completa_processed.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77439fd1",
   "metadata": {},
   "source": [
    "## Selección de métrica\n",
    "\n",
    "Se debe elegir y definir una métrica de performance a utilizar para evaluar los modelos. Fundamentar la elección de la métrica."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0e8e02",
   "metadata": {},
   "source": [
    "Con respecto a las métricas, podemos decir que vamos a utilizar recall y precisión. Esto se debe a que los datos están muy desbalanceados, y en consecuencia accuracy no sería la mejor opción porque presentaría problemas.\n",
    "\n",
    " - Recall nos va a permitir ver de todos los Foo que había, cuántos encontramos.\n",
    "\n",
    " - Precisión que nos va a permitir ver de lo que clasificamos como Foo, qué porcentaje era realmente Foo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfadfb89",
   "metadata": {},
   "source": [
    "## Aplicaciones de featuring engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05bc9cb7",
   "metadata": {},
   "source": [
    "En lo referente a feature enginering hay que hay que mencionar que vamos a aplicar la técnica de QuantileTransformer para las variables de entrada que tienen valores extremos según lo observado en el TP1. De esta forma se logra reducir el impacto de los valores atípicos. También vamos a transformar algunas variables para transformar la información de las mismas expresada de otra forma. \n",
    "Aplicamos técnicas de preprocesado para mejorar la representación de los datos como OneHotEncoder y StandardImputer, y eliminamos los valores nulos utilizando SimpleImputer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821d92c5",
   "metadata": {},
   "source": [
    "### Extraer features a partir de otras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4874af1",
   "metadata": {},
   "source": [
    "#### Agrupar equipos por regiones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8a0673",
   "metadata": {},
   "outputs": [],
   "source": [
    "east = ['TB','BAL','BOS','TOR','NYY','ATL','MIA','PHI','NYM','WSH']\n",
    "central = ['MIN','CLE','DET','CWS','KC','PIT','MIL','CHC', 'CIN', 'STL']\n",
    "west = ['TEX','LAA','HOU','SEA','OAK','LAD','SD','SF','COL','ARI']\n",
    "\n",
    "def division_h(row):\n",
    "    if east.count(row['home_team']) > 0:\n",
    "        return 'east'\n",
    "    else:\n",
    "        if central.count(row['home_team']) > 0:\n",
    "            return 'central'\n",
    "        else:\n",
    "            return 'west'\n",
    "        \n",
    "def division_a(row):\n",
    "    if east.count(row['away_team']) > 0:\n",
    "        return 'east'\n",
    "    else:\n",
    "        if central.count(row['away_team']) > 0:\n",
    "            return 'central'\n",
    "        else:\n",
    "            return 'west'\n",
    "        \n",
    "division_home = completa_fe.apply(division_h, axis=1)\n",
    "division_away = completa_fe.apply(division_a, axis=1)\n",
    "completa_fe[\"division_home\"] = division_home\n",
    "completa_fe[\"division_away\"] = division_away\n",
    "completa_fe.sample(10)\n",
    "#completa_fe[[\"division_away\",'away_team']]\n",
    "\n",
    "# Eliminar columnas que ya no son necesarias\n",
    "columnas_string=['home_team','away_team']\n",
    "completa_fe = completa_fe.drop(columnas_string,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2ed3f4",
   "metadata": {},
   "source": [
    "### Aplicación de QuantileTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1b4bbc",
   "metadata": {},
   "source": [
    "Según lo visto en el TP1, consideramos necesario verificar si esta técnica debe aplicarse a las variables \"plate_x\", \"plate_z\", \"pitch_mph\", \"launch_speed\" y \"launch_angle\". Esto se debe a que contienen valores extremos que pueden traer problemas para la predicción."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6232600e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para graficar transformaciones de datos\n",
    "def plots(df, col, t):\n",
    "    plt.figure(figsize=(12,4))\n",
    "    \n",
    "    plt.subplot(121)\n",
    "    sns.kdeplot(df[col])\n",
    "    plt.title('Antes de aplicar ' + str(t).split('(')[0])\n",
    "\n",
    "    plt.subplot(122)\n",
    "    p1 = t.fit_transform(df[[col]]).flatten()\n",
    "    sns.kdeplot(p1)\n",
    "    plt.title('Después de aplicar ' + str(t).split('(')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55767113",
   "metadata": {},
   "outputs": [],
   "source": [
    "qt = QuantileTransformer(output_distribution='normal')\n",
    "\n",
    "# Graficar variables candidatas a aplicar Quantile Transformation\n",
    "columns_qt = ['plate_x', 'plate_z', 'pitch_mph', 'launch_speed', 'launch_angle']\n",
    "for col in columns_qt:\n",
    "    plots(completa_fe, col, qt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe863f81",
   "metadata": {},
   "source": [
    "Como podemos visualizar en las gráficas, existen variables que ya presentan una distribución normal, mientras que otras presentan otro tipo de distribución.\n",
    "\n",
    "Es por ello, que decidimos aplicar la técnica en aquellas variables que lo requieren, como pitch_mph y launch_speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfdd1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reemplazamos columnas\n",
    "completa_fe['pitch_mph_qt'] = qt.fit_transform(completa_fe.pitch_mph.to_frame())\n",
    "completa_fe['launch_speed_qt'] = qt.fit_transform(completa_fe.launch_speed.to_frame())\n",
    "\n",
    "completa_fe = completa_fe.drop(['pitch_mph', 'launch_speed'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b027b86",
   "metadata": {},
   "source": [
    "### División del dataset con feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33d54eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividimos el dataset aplicando feature engine en train (60%), test (20%) y validation (20%)\n",
    "train_fe, not_train_fe = train_test_split(completa_fe, test_size=0.4, random_state=42)\n",
    "validation_fe, test_fe = train_test_split(not_train, test_size=0.5, random_state=42)\n",
    "\n",
    "train_fe.shape, validation_fe.shape, test_fe.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f22835e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Definimos el mapper. Recibe una lista de (columna/s, transformers)\n",
    "\n",
    "mapper_fe = DataFrameMapper([ \n",
    "    #(['is_batter_lefty'], None),\n",
    "    #(['is_pitcher_lefty'], None),\n",
    "    #(['bb_type'], [OneHotEncoder()]),\n",
    "    #(['bearing'], [OneHotEncoder()]),\n",
    "    #(['pitch_name'], [OneHotEncoder()]),\n",
    "    #(['inning'], [StandardScaler()]),\n",
    "    #(['outs_when_up'], None),\n",
    "    #(['balls'], [StandardScaler()]),\n",
    "    #(['strikes'], [StandardScaler()]),\n",
    "    #(['plate_x'], [StandardScaler()]),\n",
    "    #(['plate_z'], [StandardScaler()]),    \n",
    "    (['launch_angle'], [StandardScaler()]),\n",
    "    (['is_home_run'], None),\n",
    "    #(['cover'], [OneHotEncoder()]),\n",
    "    (['lf_dim'], [StandardScaler()]),\n",
    "    (['cf_dim'], [StandardScaler()]),\n",
    "    (['rf_dim'], [StandardScaler()]),\n",
    "    (['lf_w'], [StandardScaler()]),\n",
    "    (['cf_w'], [StandardScaler()]),\n",
    "    (['rf_w'], [StandardScaler()]),\n",
    "    ([\"division_home\"], [OneHotEncoder()]),\n",
    "    ([\"division_away\"], [OneHotEncoder()]),\n",
    "    (['pitch_mph_qt'], [StandardScaler()]),\n",
    "    (['launch_speed_qt'], [StandardScaler()]),\n",
    "])\n",
    "\n",
    "mapper_fe.fit(train_fe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76888b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fe.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c4bdff",
   "metadata": {},
   "source": [
    "## Modelos a utilizar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119178d6",
   "metadata": {},
   "source": [
    "Los modelos que vamos a utilizar van a ser:\n",
    " - LogisticRegression\n",
    " - KNeighborsClassifier\n",
    " - GridSearchCV\n",
    " - RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fedfefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "pipel = Pipeline([\n",
    "    ('mapper', mapper_con_fe),\n",
    "    ('imputer', IterativeImputer(random_state=42)),\n",
    "    ('classifier', LogisticRegression(random_state=42)),\n",
    "])\n",
    "\n",
    "pipel.fit(train, train.is_home_run)\n",
    "\n",
    "y_pred = pipel.predict(test)\n",
    "y_pred\n",
    "print(metrics.classification_report(test.is_home_run, y_pred))\n",
    "metrics.precision_score(test.is_home_run, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd17f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "K=200\n",
    "knn_model = Pipeline([\n",
    "    ('mapper', mapper),\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('classifier', KNeighborsClassifier(n_neighbors=K)),\n",
    "])\n",
    "\n",
    "knn_model.fit(train, train.is_home_run)\n",
    "y_pred = knn_model.predict(validation)\n",
    "y_pred\n",
    "print(metrics.classification_report(validation.is_home_run, y_pred))\n",
    "\n",
    "\n",
    "metrics.recall_score(validation.is_home_run, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdf59a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "forest_model_limits= RandomForestClassifier(n_estimators=100, max_depth=3, max_features=3, random_state=42)\n",
    "# n_estimators? max_depth=3?, max_features=2?\n",
    "\n",
    "rf_model = Pipeline([\n",
    "    ('mapper', mapper),\n",
    "    ('imputer', IterativeImputer(random_state=42)),\n",
    "    ('classifier', forest_model_limits),\n",
    "])\n",
    "\n",
    "rf_model.fit(train, train.is_home_run)\n",
    "\n",
    "y_pred = rf_model.predict(validation)\n",
    "y_pred\n",
    "print(metrics.classification_report(validation.is_home_run, y_pred))\n",
    "\n",
    "\n",
    "metrics.recall_score(validation.is_home_run, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8579f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grid Search:\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameters = {'n_neighbors': [100, 2, 50]}\n",
    "\n",
    "clf = GridSearchCV(KNeighborsClassifier, parameters, refit=True, verbose=1)\n",
    "\n",
    "gs_pipe = Pipeline([\n",
    "    ('mapper', mapper),\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('classifier', clf),\n",
    "])\n",
    "\n",
    "gs_pipe.fit(train, train.is_home_run)\n",
    "gs_pipe.predict(validation)\n",
    "#clf.best_score_, clf.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6c3859",
   "metadata": {},
   "source": [
    "Por cada modelo, se debe entrenarlo y realizar una exploración de hiper-parámetros mediante una búsqueda en grilla. Evaluar el comportamiento de cada modelo con los hiper-parámetros que mejores resultados ofrecen. En caso de ser posible, aporte conclusiones respecto a dicha comparación."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcffd1fd",
   "metadata": {},
   "source": [
    "## Técnicas de reducción de la dimensionalidad "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53d0033",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9d95aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_pipe = make_pipeline(StandardScaler(), PCA())\n",
    "pca_pipe.fit(train_b)\n",
    "\n",
    "modelo_pca = pca_pipe.named_steps['pca']\n",
    "print(train_b.columns)\n",
    "\n",
    "print('----------------------------------------------------')\n",
    "print('Porcentaje de varianza explicada por cada componente')\n",
    "print('----------------------------------------------------')\n",
    "print(modelo_pca.explained_variance_ratio_)\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(6, 4))\n",
    "ax.bar(\n",
    "    x      = np.arange(modelo_pca.n_components_) + 1,\n",
    "    height = modelo_pca.explained_variance_ratio_\n",
    ")\n",
    "\n",
    "for x, y in zip(np.arange(len(train_b.columns)) + 1, modelo_pca.explained_variance_ratio_):\n",
    "    label = round(y, 2)\n",
    "    ax.annotate(\n",
    "        label,\n",
    "        (x,y),\n",
    "        textcoords=\"offset points\",\n",
    "        xytext=(0,10),\n",
    "        ha='center'\n",
    "    )\n",
    "\n",
    "ax.set_xticks(np.arange(modelo_pca.n_components_) + 1)\n",
    "ax.set_ylim(0, 1.1)\n",
    "ax.set_title('Porcentaje de varianza explicada por cada componente')\n",
    "ax.set_xlabel('Componente principal')\n",
    "ax.set_ylabel('Por. varianza explicada');\n",
    "#train_processed.head=X_cols\n",
    "\n",
    "#pca= PCA(n_components=4,random_state=42)\n",
    "#train= pca.fit_transform(train_b)\n",
    "#[:37]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b4cf45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
