{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d91ed06",
   "metadata": {},
   "source": [
    "# Trabajo Práctico 2: Entrenamiento y evaluación de modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85887360",
   "metadata": {},
   "source": [
    "El caso de uso que se busca cubrir es el detectar si una jugada termina en home run o no cuando la bola ya fue bateada. Solo se va a poder usar dicho modelo para predecir el resultado de una jugada si se tienen los datos del lanzamiento y el bateo. Su principal uso está relacionado con el análisis de los factores que afectan a que una jugada sea home run."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5622308a",
   "metadata": {},
   "source": [
    "A partir del dataset elegido y ya preprocesado con la lógica definida en el TP1, el grupo debe realizar el entrenamiento y evaluación de al menos 3 algoritmos de machine learning.\n",
    "\n",
    "   Se debe elegir y definir una métrica de performance a utilizar para evaluar los modelos. Fundamentar la elección de la métrica.\n",
    "   Se debe aplicar alguna técnica de feature engineering para mejorar los datos de entrada a los modelos, y mostrar la comparativa de los resultados obtenidos en cada caso. Si no es posible o útil, fundamentar el motivo por el cual no se realizará.\n",
    "   Por cada modelo, se debe entrenarlo y realizar una exploración de hiper-parámetros mediante una búsqueda en grilla. Evaluar el comportamiento de cada modelo con los hiper-parámetros que mejores resultados ofrecen. En caso de ser posible, aporte conclusiones respecto a dicha comparación.\n",
    "   Realizar experimentos que utilicen como datos de entrada representaciones intermedias de los datos (generadas por técnicas de reducción de dimensiones como PCA). Compare los resultados obtenidos contra los casos previos, interprete y proponga conclusiones.\n",
    "   Se deben utilizar técnicas que garanticen que los modelos no están sobreentrenando sin que nos demos cuenta.\n",
    "   Determinar el valor final de la métrica que podría ser informado al cliente, utilizando técnicas que permitan obtener un valor lo más realista posible. Fundamentar y considerar no solo el rendimiento del modelo en su elección, sino también cuestiones como interpretabilidad, tiempos de entrenamiento, etc.\n",
    "   Para el método propuesto como definitivo, y para distintos pares de variables, genere diagramas de dispersión donde se visualicen los aciertos y errores del mismo. Discuta si existen patrones o conocimiento que se pueda obtener a partir de dichos errores. En caso de ser posible, evalúe la importancia que asigna el método a las variables de entrada y genere conclusiones al respecto.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda09d4d",
   "metadata": {},
   "source": [
    "# Configuración inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e2fc07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos las dependencias necesarias.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "import warnings, time\n",
    "from sklearn.pipeline import Pipeline\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import SimpleImputer, IterativeImputer\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, OneHotEncoder\n",
    "from sklearn import metrics\n",
    "\n",
    "#from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import graphviz  \n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d187f741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arreglamos el dataset según lo establecido en el TP1\n",
    "\n",
    "# Importamos train.csv y park_dimensions.csv, los unimos utilizando la variable \"park\"\n",
    "entrenamiento = pd.read_csv('./train.csv')\n",
    "estadio = pd.read_csv('./park_dimensions.csv')\n",
    "completa=entrenamiento.merge(estadio, on=\"park\", how=\"left\")\n",
    "\n",
    "# Desechamos las variables no utilizadas\n",
    "completa = completa.drop(['park','bip_id','batter_id','pitcher_id'],axis=1)\n",
    "\n",
    "# Asignamos nuevos nombres a las columnas\n",
    "renamed_columns = {'NAME': 'name', 'Cover': 'cover', 'LF_Dim': 'lf_dim', 'CF_Dim':'cf_dim',\n",
    "                   'RF_Dim': 'rf_dim', 'LF_W': 'lf_w', 'CF_W': 'cf_w', 'RF_W': 'rf_w'\n",
    "                  }\n",
    "completa.rename(columns=renamed_columns, inplace=True)\n",
    "\n",
    "# Convertir columna \"game_date\" de tipo object/string, a datetime\n",
    "completa['game_date'] = pd.to_datetime(completa['game_date'])\n",
    "\n",
    "# Eliminar datos filas con datos nulos en bb_type\n",
    "completa = completa[~completa.bb_type.isnull()]\n",
    "\n",
    "# Delimitación de conjuntos\n",
    "completa.isnull().sum()\n",
    "\n",
    "# Crear df a la que se le va aplicar feature engineering\n",
    "completa_fe = completa\n",
    "columnas_string=['game_date', 'batter_team','batter_name','pitcher_name','name']\n",
    "completa_fe = completa_fe.drop(columnas_string,axis=1)\n",
    "\n",
    "# Eliminar columnas cuyos valores sean cadenas\n",
    "columnas_string=['home_team','away_team', 'game_date', 'batter_team','batter_name','pitcher_name','name']\n",
    "completa = completa.drop(columnas_string,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252a5b92",
   "metadata": {},
   "source": [
    "## División del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26736c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividimos el dataset en train (60%), test (20%) y validation (20%)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, not_train = train_test_split(completa, test_size=0.4, random_state=42)\n",
    "validation, test = train_test_split(not_train, test_size=0.5, random_state=42)\n",
    "\n",
    "train.shape, validation.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb18c1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos el mapper. Recibe una lista de (columna/s, transformers)\n",
    "\n",
    "mapper = DataFrameMapper([ \n",
    "    (['is_batter_lefty'], None),#\n",
    "    (['is_pitcher_lefty'], None),#\n",
    "    (['bb_type'], [OneHotEncoder()]),#\n",
    "    (['bearing'], [OneHotEncoder()]),#\n",
    "    (['pitch_name'], [OneHotEncoder()]),#\n",
    "    (['inning'], [StandardScaler()]),#\n",
    "    (['outs_when_up'], None),#\n",
    "    (['balls'], [StandardScaler()]),#\n",
    "    (['strikes'], [StandardScaler()]),#\n",
    "    (['plate_x'], [StandardScaler()]),#\n",
    "    (['plate_z'], [StandardScaler()]),#\n",
    "    (['pitch_mph'], [StandardScaler()]),\n",
    "    (['launch_speed'], [StandardScaler()]),\n",
    "    (['launch_angle'], [StandardScaler()]),\n",
    "    (['cover'], [OneHotEncoder()]),#\n",
    "    (['lf_dim'], [StandardScaler()]),\n",
    "    (['cf_dim'], [StandardScaler()]),\n",
    "    (['rf_dim'], [StandardScaler()]),\n",
    "    (['lf_w'], [StandardScaler()]),\n",
    "    (['cf_w'], [StandardScaler()]),\n",
    "    (['rf_w'], [StandardScaler()]),\n",
    "])\n",
    "\n",
    "train_nor=train\n",
    "mapper.fit(train_nor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77439fd1",
   "metadata": {},
   "source": [
    "# Selección de métrica"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0e8e02",
   "metadata": {},
   "source": [
    "Decidimos utilizar Recall y Precision, debido a que los valores de la variable de salida no se encuentran balanceados.\n",
    "- Precision nos permite comprender el rendimiento de un clasificador con respecto a los falsos positivos (de los que clasificamos como Foo, qué porcentaje era realmente Foo).\n",
    "- Recall nos permite comprender el rendimiento de un clasificador con respecto a falsos negativos (de todos los Foo que había, qué porcentaje era realmente Foo).\n",
    "\n",
    "Al utilizar las dos, no solo ponderamos que el modelo tenga una precisión aceptable, sino que también consideramos cuántos casos está encontrando realmente (ya que, el modelo como tal puede no resultar tan útil si lo hace con muy pocos)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfadfb89",
   "metadata": {},
   "source": [
    "# Aplicaciones de featuring engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05bc9cb7",
   "metadata": {},
   "source": [
    "Nuestro dataset posee algunas variables a las que podemos aplicarles \"feature engineering\". Decidimos crear \"division_home\" y \"division_away\" a partir de los datos en las variables \"home_team\" y \"away_team\", de las cuales extraeremos la región a las que pertenecen los equipos en cuestión.\n",
    "\n",
    "Por otra parte, aplicaremos la técnica de \"Quantile Transformation\" para ajustar a una distribución normal aquellas variables de entrada que presentan otro tipo de distribución, eliminando los valores atípicos.\n",
    "\n",
    "Aplicamos técnicas de preprocesado para mejorar la representación de los datos como OneHotEncoder y StandardImputer, y eliminamos los valores nulos utilizando SimpleImputer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821d92c5",
   "metadata": {},
   "source": [
    "## Extraer features a partir de otras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8a0673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regiones\n",
    "east = ['TB','BAL','BOS','TOR','NYY','ATL','MIA','PHI','NYM','WSH']\n",
    "central = ['MIN','CLE','DET','CWS','KC','PIT','MIL','CHC', 'CIN', 'STL']\n",
    "west = ['TEX','LAA','HOU','SEA','OAK','LAD','SD','SF','COL','ARI']\n",
    "\n",
    "def division_h(row):\n",
    "    if east.count(row['home_team']) > 0:\n",
    "        return 'east'\n",
    "    else:\n",
    "        if central.count(row['home_team']) > 0:\n",
    "            return 'central'\n",
    "        else:\n",
    "            return 'west'\n",
    "        \n",
    "def division_a(row):\n",
    "    if east.count(row['away_team']) > 0:\n",
    "        return 'east'\n",
    "    else:\n",
    "        if central.count(row['away_team']) > 0:\n",
    "            return 'central'\n",
    "        else:\n",
    "            return 'west'\n",
    "\n",
    "# Nuevas features que almacenan la región del equipo local y visitante\n",
    "division_home = completa_fe.apply(division_h, axis=1)\n",
    "division_away = completa_fe.apply(division_a, axis=1)\n",
    "completa_fe[\"division_home\"] = division_home\n",
    "completa_fe[\"division_away\"] = division_away\n",
    "\n",
    "# Eliminar features que ya no son necesarias\n",
    "columnas_string=['home_team','away_team']\n",
    "completa_fe = completa_fe.drop(columnas_string,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2ed3f4",
   "metadata": {},
   "source": [
    "## Aplicación de QuantileTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1b4bbc",
   "metadata": {},
   "source": [
    "Las features candidatas para aplicar \"Quantile Transformation\" son \"plate_x\", \"plate_z\", \"pitch_mph\", \"launch_speed\" y \"launch_angle\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6232600e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para graficar variables, antes y después de aplicar alguna transformación\n",
    "def plots(df, col, t):\n",
    "    plt.figure(figsize=(12,4))\n",
    "    \n",
    "    plt.subplot(121)\n",
    "    sns.kdeplot(df[col])\n",
    "    plt.title('Antes de aplicar ' + str(t).split('(')[0])\n",
    "\n",
    "    plt.subplot(122)\n",
    "    p1 = t.fit_transform(df[[col]]).flatten()\n",
    "    sns.kdeplot(p1)\n",
    "    plt.title('Después de aplicar ' + str(t).split('(')[0])\n",
    "\n",
    "# Gráficas de variables, antes y después de aplicar Quantile Transformation\n",
    "columns_qt = ['plate_x', 'plate_z', 'pitch_mph', 'launch_speed', 'launch_angle']\n",
    "for col in columns_qt:\n",
    "    plots(completa_fe, col, QuantileTransformer(output_distribution='normal'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe863f81",
   "metadata": {},
   "source": [
    "Como podemos visualizar en las gráficas, existen variables que ya presentan una distribución normal, mientras que otras presentan otro tipo de distribución. Es por ello, que decidimos aplicar la técnica en aquellas variables que lo requieren, como pitch_mph y launch_speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfdd1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "qt = QuantileTransformer(output_distribution='normal')\n",
    "\n",
    "# Creamos columnas con la técnica aplicada\n",
    "completa_fe['pitch_mph_qt'] = qt.fit_transform(completa_fe.pitch_mph.to_frame())\n",
    "completa_fe['launch_speed_qt'] = qt.fit_transform(completa_fe.launch_speed.to_frame())\n",
    "\n",
    "# Eliminar features que ya no son necesarias\n",
    "completa_fe = completa_fe.drop(['pitch_mph', 'launch_speed'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b027b86",
   "metadata": {},
   "source": [
    "## División del dataset con feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33d54eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividimos el dataset aplicando feature engine en train (60%), test (20%) y validation (20%)\n",
    "train_fe, not_train_fe = train_test_split(completa_fe, test_size=0.4, random_state=42)\n",
    "validation_fe, test_fe = train_test_split(not_train_fe, test_size=0.5, random_state=42)\n",
    "\n",
    "train_fe.shape, validation_fe.shape, test_fe.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f22835e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Definimos el mapper. Recibe una lista de (columna/s, transformers)\n",
    "mapper_fe = DataFrameMapper([ \n",
    "    (['is_batter_lefty'], None), #\n",
    "    (['is_pitcher_lefty'], None), #\n",
    "    (['bb_type'], [OneHotEncoder()]), #\n",
    "    (['bearing'], [OneHotEncoder()]), #\n",
    "    (['pitch_name'], [OneHotEncoder()]), #\n",
    "    (['inning'], [StandardScaler()]), #\n",
    "    (['outs_when_up'], None), #\n",
    "    (['balls'], [StandardScaler()]), #\n",
    "    (['strikes'], [StandardScaler()]), #\n",
    "    (['plate_x'], [StandardScaler()]), #\n",
    "    (['plate_z'], [StandardScaler()]), #   \n",
    "    (['launch_angle'], [StandardScaler()]),\n",
    "    (['cover'], [OneHotEncoder()]),#\n",
    "    (['lf_dim'], [StandardScaler()]),\n",
    "    (['cf_dim'], [StandardScaler()]),\n",
    "    (['rf_dim'], [StandardScaler()]),\n",
    "    (['lf_w'], [StandardScaler()]),\n",
    "    (['cf_w'], [StandardScaler()]),\n",
    "    (['rf_w'], [StandardScaler()]),\n",
    "    ([\"division_home\"], [OneHotEncoder()]),\n",
    "    ([\"division_away\"], [OneHotEncoder()]),\n",
    "    (['pitch_mph_qt'], [StandardScaler()]),\n",
    "    (['launch_speed_qt'], [StandardScaler()]),\n",
    "])\n",
    "\n",
    "mapper_fe.fit(train_fe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c4bdff",
   "metadata": {},
   "source": [
    "# Modelos a utilizar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119178d6",
   "metadata": {},
   "source": [
    "Los modelos que vamos a utilizar van a ser:\n",
    " - K-Nearest Neighbors (?)\n",
    " - Árboles de decisión\n",
    " - Random Forest\n",
    " - Gradient Boost\n",
    " \n",
    "  Por cada modelo, se debe entrenarlo y realizar una exploración de hiper-parámetros mediante una búsqueda en grilla. Evaluar el comportamiento de cada modelo con los hiper-parámetros que mejores resultados ofrecen. En caso de ser posible, aporte conclusiones respecto a dicha comparación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997cf598",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para evaluar modelos (sin aplicar feature engineering)\n",
    "def evaluate_model(model, set_names=('train_nor', 'validation'), title='', show_cm=False):\n",
    "    if title:\n",
    "        display(title)\n",
    "    \n",
    "    final_metrics = {\n",
    "        'Precision': [],\n",
    "        'Recall': [],\n",
    "        'F1': [],        \n",
    "    }\n",
    "        \n",
    "    for i, set_name in enumerate(set_names):\n",
    "        assert set_name in ['train_nor', 'validation', 'test']\n",
    "        set_data = globals()[set_name]\n",
    "\n",
    "        y = set_data.is_home_run\n",
    "        y_pred = model.predict(set_data)\n",
    "        final_metrics['Precision'].append(metrics.precision_score(y, y_pred))\n",
    "        final_metrics['Recall'].append(metrics.recall_score(y, y_pred))\n",
    "        final_metrics['F1'].append(metrics.f1_score(y, y_pred))\n",
    "        \n",
    "        if show_cm:\n",
    "            cm = metrics.confusion_matrix(y, y_pred)\n",
    "            cm_plot = metrics.ConfusionMatrixDisplay(confusion_matrix=cm, \n",
    "                                                     display_labels=['Es', 'No es'])\n",
    "            cm_plot.plot(cmap=\"Blues\")\n",
    "        \n",
    "    display(pd.DataFrame(final_metrics, index=set_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d7f402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para evaluar modelos (aplicando feature engineering)    \n",
    "def evaluate_model_fe(model, set_names=('train_fe', 'validation_fe'), title='', show_cm=False):\n",
    "    if title:\n",
    "        display(title)\n",
    "        \n",
    "    final_metrics = {\n",
    "        'Precision': [],\n",
    "        'Recall': [],\n",
    "        'F1': [],        \n",
    "    }\n",
    "        \n",
    "    for i, set_name in enumerate(set_names):\n",
    "        assert set_name in ['train_fe', 'validation_fe', 'test_fe']\n",
    "        set_data = globals()[set_name]  # <- hack feo...\n",
    "\n",
    "        y = set_data.is_home_run\n",
    "        y_pred = model.predict(set_data)\n",
    "        final_metrics['Precision'].append(metrics.precision_score(y, y_pred))\n",
    "        final_metrics['Recall'].append(metrics.recall_score(y, y_pred))\n",
    "        final_metrics['F1'].append(metrics.f1_score(y, y_pred))\n",
    "        \n",
    "        if show_cm:\n",
    "            cm = metrics.confusion_matrix(y, y_pred)\n",
    "            cm_plot = metrics.ConfusionMatrixDisplay(confusion_matrix=cm, \n",
    "                                                     display_labels=['Es', 'No es'])\n",
    "            cm_plot.plot(cmap=\"Blues\")\n",
    "        \n",
    "    display(pd.DataFrame(final_metrics, index=set_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bac004e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gráfico del árbol\n",
    "def graph_tree(tree, col_names):\n",
    "    graph_data = export_graphviz(\n",
    "        tree, \n",
    "        out_file=None, \n",
    "        feature_names=col_names,  \n",
    "        class_names=['No es home run', 'Home run'],  \n",
    "        filled=True, \n",
    "        rounded=True,  \n",
    "        special_characters=True,\n",
    "    )\n",
    "    graph = graphviz.Source(graph_data)  \n",
    "    return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5c7dee",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de43145f",
   "metadata": {},
   "source": [
    "### Exploración de hiper-parámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3909c9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Grid Search para exploración de hiper-parámetros\n",
    "\n",
    "k_range = list(range(1, 31))\n",
    "knn = KNeighborsClassifier()\n",
    "param_grid = dict(n_neighbors=k_range)\n",
    "clf = GridSearchCV(knn, param_grid, scoring='f1', verbose=1)\n",
    "\n",
    "gs_pipe = Pipeline([\n",
    "    ('mapper', mapper),\n",
    "    ('imputer', IterativeImputer(random_state=42)),\n",
    "    ('classifier', clf),\n",
    "])\n",
    "\n",
    "gs_pipe.fit(train_nor, train_nor.is_home_run)\n",
    "clf.best_score_, clf.best_params_\n",
    "\n",
    "# Out: (0.19843055574805257, {'n_neighbors': 1})\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf99d421",
   "metadata": {},
   "source": [
    "### Sin feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b7f4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Entrenamiento\n",
    "k=5\n",
    "knn_model = Pipeline([\n",
    "    ('mapper', mapper),\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('classifier', KNeighborsClassifier(n_neighbors=k)),\n",
    "])\n",
    "inicio = time.time()\n",
    "knn_model.fit(train_nor, train_nor.is_home_run)\n",
    "fin = time.time()\n",
    "print(f\"Demora entrenamiento: {fin - inicio}s\")\n",
    "\n",
    "# Evaluación\n",
    "evaluate_model(knn_model, title='KNN')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b35c3f",
   "metadata": {},
   "source": [
    "### Con feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19f4f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Entrenamiento\n",
    "k=5\n",
    "knn_model_fe = Pipeline([\n",
    "    ('mapper', mapper_fe),\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('classifier', KNeighborsClassifier(n_neighbors=k)),\n",
    "])\n",
    "inicio = time.time()\n",
    "knn_model_fe.fit(train_fe, train_fe.is_home_run)\n",
    "fin = time.time()\n",
    "print(f\"Demora entrenamiento: {fin - inicio}s\")\n",
    "\n",
    "# Evaluación\n",
    "evaluate_model_fe(knn_model_fe, title='KNN (aplicando feature engineering)')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9554cfe",
   "metadata": {},
   "source": [
    "## Árboles de decisión"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac10db79",
   "metadata": {},
   "source": [
    "### Exploración de hiper-parámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033964b5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "profundidad = list(range(4, 12))\n",
    "param_grid = dict(max_depth=profundidad)\n",
    "clf = GridSearchCV(DecisionTreeClassifier(random_state=1), param_grid, scoring='f1', verbose=1)\n",
    "\n",
    "gs_pipe = Pipeline([\n",
    "    ('mapper', mapper),\n",
    "    ('imputer', IterativeImputer(random_state=42)),\n",
    "    ('classifier', clf),\n",
    "])\n",
    "\n",
    "gs_pipe.fit(train_nor, train_nor.is_home_run)\n",
    "clf.best_score_, clf.best_params_\n",
    "\"\"\"\n",
    "# Out: (0.5030525830032249, {'max_depth': 5})\n",
    "\n",
    "tree_params = {'max_depth': 5}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0574acaf",
   "metadata": {},
   "source": [
    "### Sin feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a109759",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_model_limit = DecisionTreeClassifier(**tree_params, random_state=42)\n",
    "\n",
    "# Entrenamiento\n",
    "dt_model = Pipeline([\n",
    "    ('mapper', mapper),\n",
    "    ('imputer', IterativeImputer(random_state=42)),\n",
    "    ('classifier', tree_model_limit),\n",
    "])\n",
    "inicio = time.time()\n",
    "dt_model.fit(train_nor, train_nor.is_home_run)\n",
    "fin = time.time()\n",
    "print(f\"Demora entrenamiento: {fin - inicio}s\")\n",
    "\n",
    "# Evaluación\n",
    "evaluate_model(dt_model, title='Árbol de decisión')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9c0170",
   "metadata": {},
   "source": [
    "### Con feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1076cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_model_limit_fe = DecisionTreeClassifier(**tree_params, random_state=42)\n",
    "\n",
    "# Entrenamiento\n",
    "dt_model_fe = Pipeline([\n",
    "    ('mapper', mapper_fe),\n",
    "    ('imputer', IterativeImputer(random_state=42)),\n",
    "    ('classifier', tree_model_limit_fe),\n",
    "])\n",
    "inicio = time.time()\n",
    "dt_model_fe.fit(train_fe, train_fe.is_home_run)\n",
    "fin = time.time()\n",
    "dt_time_train = fin - inicio\n",
    "print(f\"Demora entrenamiento: {dt_time_train}s\")\n",
    "\n",
    "# Evaluación\n",
    "evaluate_model_fe(dt_model_fe, title='Árbol de decisión (aplicando feature engineering)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18802431",
   "metadata": {},
   "source": [
    "### Conclusión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f2f270",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graficar el árbol de decisión entrenado\n",
    "graph_tree(tree_model_limit, mapper.transformed_names_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30fb85b6",
   "metadata": {},
   "source": [
    "A partir de los resultados, podemos determinar que:\n",
    "- El 72% de las ocasiones en las que el modelo predijo un home run, acertó.\n",
    "- El modelo (con la configuración planteada) es capaz de encontrar el 39% de las jugadas que terminaron en home run.\n",
    "- Debido a la diferencia entre Precision y Recall, la configuración usada para este modelo nos devuelve un F1 de aproximadamente 50%.\n",
    "- Con lo anterior, concluimos que el modelo no detecta muy bien las jugadas que son home run, pero cuando lo hace es confiable.\n",
    "- En lo que respecta a la aplicación de feature engineering, los valores de las métricas se ven afectados negativamente.\n",
    "- El árbol no es excesivamente grande para visualizarse, por lo que resulta más simple de entender e interpretar a diferencia de otros modelos. Podemos visualizar que las variables que tienen más impacto en las predicciones son  launch_angle y launch_speed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dadef57a",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b76eb90",
   "metadata": {},
   "source": [
    "### Exploración de hiper-parámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c00d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "parameters = {'n_estimators': [100, 200], \n",
    "              'max_depth':[3, 5, 8],\n",
    "              'max_features': [2, 5]}\n",
    "clf = GridSearchCV(RandomForestClassifier(random_state=42), parameters, scoring='f1', verbose=1)\n",
    "\n",
    "gs_pipe = Pipeline([\n",
    "    ('mapper', mapper),\n",
    "    ('imputer', IterativeImputer(random_state=42)),\n",
    "    ('classifier', clf),\n",
    "])\n",
    "\n",
    "gs_pipe.fit(train_nor, train_nor.is_home_run)\n",
    "clf.best_score_, clf.best_params_\n",
    "\"\"\"\n",
    "# Out: (0.25394347435005404, {'max_depth': 8, 'max_features': 5, 'n_estimators': 200})\n",
    "\n",
    "rf_params = {'max_depth': 8,\n",
    "            'max_features': 5,\n",
    "            'n_estimators': 200\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d09452d",
   "metadata": {},
   "source": [
    "### Sin feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a20660",
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_model = RandomForestClassifier(**rf_params, random_state=42)\n",
    "\n",
    "# Entrenamiento\n",
    "rf_model = Pipeline([\n",
    "    ('mapper', mapper),\n",
    "    ('imputer', IterativeImputer(random_state=42)),\n",
    "    ('classifier', forest_model),\n",
    "])\n",
    "inicio = time.time()\n",
    "rf_model.fit(train_nor, train_nor.is_home_run)\n",
    "fin = time.time()\n",
    "print(f\"Demora entrenamiento: {fin - inicio}s\")\n",
    "\n",
    "# Evaluación\n",
    "evaluate_model(rf_model, title='Random Forest')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230bd65a",
   "metadata": {},
   "source": [
    "### Con feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2642bcc3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "forest_model_fe = RandomForestClassifier(**rf_params, random_state=42)\n",
    "\n",
    "# Entrenamiento\n",
    "rf_model_fe = Pipeline([\n",
    "    ('mapper', mapper_fe),\n",
    "    ('imputer', IterativeImputer(random_state=42)),\n",
    "    ('classifier', forest_model_fe),\n",
    "])\n",
    "inicio = time.time()\n",
    "rf_model_fe.fit(train_fe, train_fe.is_home_run)\n",
    "fin = time.time()\n",
    "rf_time_train = fin - inicio\n",
    "print(f\"Demora entrenamiento: {rf_time_train}s\")\n",
    "\n",
    "# Evaluación\n",
    "evaluate_model_fe(rf_model_fe, title='Random Forest (aplicando feature engineering)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6c3f47",
   "metadata": {},
   "source": [
    "### Conclusión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23cc2748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graficar el árbol de decisión entrenado\n",
    "graph_tree(forest_model.estimators_[0], mapper.transformed_names_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0410098a",
   "metadata": {},
   "source": [
    "A partir de los resultados, podemos determinar que:\n",
    "\n",
    "- La Precision obtenida a partir de los dos conjuntos de entrenamiento nos indica que el modelo está sobreentrenando.\n",
    "- El 80% de las ocasiones en las que el modelo predijo un home run, acertó.\n",
    "- El modelo (con la configuración planteada) es capaz de encontrar el 20% de las jugadas que terminaron en home run.\n",
    "- Debido a la diferencia entre Precision y Recall, la configuración usada para este modelo nos devuelve un F1 de aproximadamente 31%.\n",
    "- Con lo anterior, concluimos que el modelo no detecta muy bien las jugadas que son home run, pero cuando lo hace es muy confiable.\n",
    "- En lo que respecta a la aplicación de feature engineering, los valores de las métricas se ven afectados negativamente en F1 y positivamente en Precision.\n",
    "- El árbol es excesivamente grande y complejo, por lo que entenderlo e interpretarlo se dificulta. A pesar de lo anterior, podemos visualizar que las variables que tienen más impacto en las predicciones son  launch_angle, launch_speed y plate_x."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7021a2a",
   "metadata": {},
   "source": [
    "## Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313cda9a",
   "metadata": {},
   "source": [
    "### Exploración de hiper-parámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31474796",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "e_range = [10, 50, 100, 200]\n",
    "param_grid = dict(n_estimators=e_range)\n",
    "clf = GridSearchCV(GradientBoostingClassifier(), param_grid, scoring= 'f1',refit=True,verbose=1)\n",
    "\n",
    "gs_pipe = Pipeline([\n",
    "    ('mapper', mapper),\n",
    "    ('imputer', IterativeImputer(random_state=42)),\n",
    "    ('classifier', clf),\n",
    "])\n",
    "\n",
    "gs_pipe.fit(train_nor, train_nor.is_home_run)\n",
    "clf.best_score_, clf.best_params_\n",
    "\"\"\"\n",
    "# Out: (0.5265112649454026, {'n_estimators': 100})\n",
    "\n",
    "gb_params = {'n_estimators':100}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57031c30",
   "metadata": {},
   "source": [
    "### Sin feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fedfefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_model = GradientBoostingClassifier(**gb_params, random_state=42)\n",
    "\n",
    "# Entrenamiento\n",
    "model_gb100 = Pipeline([\n",
    "    ('mapper', mapper),\n",
    "    ('imputer', IterativeImputer(random_state=42)),\n",
    "    ('classifier', gb_model)\n",
    "])\n",
    "inicio = time.time()\n",
    "model_gb100.fit(train_nor, train_nor.is_home_run)\n",
    "fin = time.time()\n",
    "gb_time_train = fin - inicio\n",
    "print(f\"Demora entrenamiento: {gb_time_train}s\")\n",
    "\n",
    "# Evaluación\n",
    "evaluate_model(model_gb100, title='Gradient Boosting con n_trees=100')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c3f6bf",
   "metadata": {},
   "source": [
    "### Con feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c58032c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_model_fe = GradientBoostingClassifier(**gb_params, random_state=42)\n",
    "\n",
    "# Entrenamiento\n",
    "model_gb100_fe = Pipeline([\n",
    "    ('mapper', mapper_fe),\n",
    "    ('imputer', IterativeImputer(random_state=42)),\n",
    "    ('classifier', gb_model_fe)\n",
    "])\n",
    "inicio = time.time()\n",
    "model_gb100_fe.fit(train_fe, train_fe.is_home_run)\n",
    "fin = time.time()\n",
    "print(f\"Demora entrenamiento: {fin - inicio}s\")\n",
    "\n",
    "# Evaluación\n",
    "evaluate_model_fe(model_gb100_fe, title='Gradient Boosting con n_trees=100 (aplicando feature engineering)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7fd8f3",
   "metadata": {},
   "source": [
    "### Conclusión"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1f38ae",
   "metadata": {},
   "source": [
    "A partir de los resultados, podemos determinar que:\n",
    "\n",
    "- El 71% de las ocasiones en las que el modelo predijo un home run, acertó.\n",
    "- El modelo (con la configuración planteada) es capaz de encontrar el 41% de las jugadas que terminaron en home run.\n",
    "- Debido a la diferencia entre Precision y Recall, la configuración usada para este modelo nos devuelve un F1 de aproximadamente 52%.\n",
    "- Con lo anterior, concluimos que el modelo no detecta muy bien las jugadas que son home run, pero cuando lo hace es confiable.\n",
    "- En lo que respecta a la aplicación de feature engineering, los valores de las métricas se ven afectados negativamente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcffd1fd",
   "metadata": {},
   "source": [
    "# Técnicas de reducción de la dimensionalidad "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b16863b",
   "metadata": {},
   "source": [
    "El análisis de componentes principales (PCA) es una técnica utilizada para describir un conjunto de datos en términos de nuevas variables no correlacionadas, permite hacer dimensionality reduction. El algoritmo seleccionará el número de componentes conservando un porcentaje de la varianza en los datos, el cual podremos especificar en \"n_components\". Por lo general, se suele utilizar un valor de 0.95."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1989e16c",
   "metadata": {},
   "source": [
    "## Árboles de decisión con PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf3155d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_model_limit_pca = DecisionTreeClassifier(**tree_params, random_state=42)\n",
    "\n",
    "# Entrenamiento\n",
    "dt_model_pca = Pipeline([\n",
    "    ('mapper', mapper),\n",
    "    ('imputer', IterativeImputer(random_state=42)),\n",
    "    ('pca', PCA(n_components=0.95)),\n",
    "    ('classifier', tree_model_limit_pca),\n",
    "])\n",
    "start = time.time()\n",
    "dt_model_pca.fit(train_nor, train_nor.is_home_run)\n",
    "stop = time.time()\n",
    "print(f\"Demora entrenamiento: {stop - start}s\")\n",
    "\n",
    "# Evaluación\n",
    "evaluate_model(dt_model_pca, title='Árbol de decisión (aplicando PCA)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e9ae36",
   "metadata": {},
   "source": [
    "## Random Forest con PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87d529c",
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_model_pca = RandomForestClassifier(**rf_params, random_state=42)\n",
    "\n",
    "# Entrenamiento\n",
    "rf_model_pca = Pipeline([\n",
    "    ('mapper', mapper),\n",
    "    ('imputer', IterativeImputer(random_state=42)),\n",
    "    ('pca', PCA(n_components=0.95)),\n",
    "    ('classifier', forest_model_pca),\n",
    "])\n",
    "inicio = time.time()\n",
    "rf_model_pca.fit(train_nor, train_nor.is_home_run)\n",
    "fin = time.time()\n",
    "print(f\"Demora entrenamiento: {fin - inicio}s\")\n",
    "\n",
    "# Evaluación\n",
    "evaluate_model(rf_model_pca, title='Random Forest (aplicando PCA)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01949177",
   "metadata": {},
   "source": [
    "## Gradient Boost con PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c216cca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_model_pca = GradientBoostingClassifier(**gb_params, random_state=42)\n",
    "\n",
    "# Entrenamiento\n",
    "model_gb100_pca = Pipeline([\n",
    "    ('mapper', mapper),\n",
    "    ('imputer', IterativeImputer(random_state=42)),\n",
    "    ('pca', PCA(n_components=0.95)),\n",
    "    ('classifier', gb_model_pca),\n",
    "])\n",
    "inicio = time.time()\n",
    "model_gb100_pca.fit(train_nor, train_nor.is_home_run)\n",
    "fin = time.time()\n",
    "print(f\"Demora entrenamiento: {fin - inicio}s\")\n",
    "\n",
    "# Evaluación\n",
    "evaluate_model(model_gb100_pca, title='Gradient Boosting con n_trees=100 (aplicando PCA)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160bfd12",
   "metadata": {},
   "source": [
    "## Conclusión"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ce6023",
   "metadata": {},
   "source": [
    "PCA permite resumir la información de un gran número de features en un número limitado de componentes, pero dichos componentes no suelen ser intuitivos. Comparandolo con los modelos anteriores que no aplican PCA, concluimos que en nuestro caso no mejora el rendimiento de la predicción (además, sobreentrena), la razón seguramente se deba a la pérdida de información que implica el reducir la dimensión del conjunto de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb596ea7",
   "metadata": {},
   "source": [
    "# Técnicas para evitar overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ece4e3d",
   "metadata": {},
   "source": [
    "- Dividimos los datasets (con y sin feature engineering) en train, test y validation. Esto nos permite obtener una valoración de aciertos/fallos del modelo, y compararlos entre subconjuntos para detectar fácilmente overfitting/underfitting.\n",
    "- Modificamos los hiperparámetros en los modelos utilizados para que no sobreentrene. Por ejemplo, al establecer una profundidad de árbol y, de esa forma, evitar que el árbol de decisión se ajuste perfectamente al set de entrenamiento.\n",
    "- Una mayor cantidad de datos permite que el modelo pueda generalizar mejor, teniendo en cuenta más tipos de datos, en nuestro caso podríamos utilizar datasets de otras temporadas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037408eb",
   "metadata": {},
   "source": [
    "# Selección del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e245f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(dt_model, title='Árboles de decisión', set_names=('train_nor', 'validation','test'))\n",
    "print(f\"Demora entrenamiento: {dt_time_train}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1831695f",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(rf_model, title='Random Forest', set_names=('train_nor', 'validation','test'))\n",
    "print(f\"Demora entrenamiento: {rf_time_train}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b915694",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(model_gb100, title='Gradient Boosting con n_trees=100', set_names=('train_nor', 'validation','test'))\n",
    "print(f\"Demora entrenamiento: {gb_time_train}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21420b7",
   "metadata": {},
   "source": [
    "Los modelos candidatos son árboles de decisión y Gradient Boosting, ya que presentan los valores más altos en F1 (que considera tanto la Precision como el Recall) utilizando el conjunto de datos \"validation\". Además, los tiempos de entrenamiento de ambos son similares y la diferencia entre el rendimiento del conjunto \"train\" y \"validation\" no es tan amplia en ninguno de los dos casos. Al final, optamos por el modelo de árboles de decisión debido a que los mismos pueden resultar más simples e intuitivos para que el cliente pueda interpretarlos.\n",
    "\n",
    "La métrica final que será informada la conseguimos a partir del conjunto de datos \"test\", para obtener un valor lo más realista posible. De esta forma, al momento de informar al cliente acerca del rendimiento del modelo, se le comunica que posee una precisión del 68% y que es capaz de identificar el 48% de los casos en que una jugada termina en home run."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99aee628",
   "metadata": {},
   "source": [
    "# Diagramas de dispersión con aciertos y errores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4b8aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=train_nor[], y=, hue=train_nor.is_home_run, data=train_nor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4623745",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mapper_pca = DataFrameMapper([ \n",
    "    (['is_batter_lefty'], None), #\n",
    "    (['bb_type'], [OneHotEncoder()]),\n",
    "    (['is_pitcher_lefty'], None), #\n",
    "    (['bearing'], [OneHotEncoder()]), #\n",
    "    (['inning'], [StandardScaler()]), #\n",
    "    (['outs_when_up'], None), #\n",
    "    (['balls'], [StandardScaler()]), #\n",
    "    (['strikes'], [StandardScaler()]), #\n",
    "    (['plate_x'], [StandardScaler()]), #\n",
    "    (['plate_z'], [StandardScaler()]), #   \n",
    "    (['launch_angle'], [StandardScaler()]), # sacar los valores nulos para que sea válido\n",
    "    (['launch_speed'], [StandardScaler()]),\n",
    "    (['pitch_mph'], [StandardScaler()]),\n",
    "    (['cover'], [OneHotEncoder()]),\n",
    "    (['lf_dim'], [StandardScaler()]),\n",
    "    (['cf_dim'], [StandardScaler()]),\n",
    "    (['rf_dim'], [StandardScaler()]),\n",
    "    (['lf_w'], [StandardScaler()]),\n",
    "    (['cf_w'], [StandardScaler()]),\n",
    "    (['rf_w'], None),\n",
    "    #(['is_home_run'], None),\n",
    "])\n",
    "\n",
    "mapper_pca.fit(train)\n",
    "\n",
    "pipe_pca1 = Pipeline([\n",
    "    ('mapper', mapper_pca),\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "])\n",
    "\n",
    "# preparar los datos\n",
    "pipe_pca1.fit(train)\n",
    "train_pca=pipe_pca1.transform(train)\n",
    "train_pca= pd.DataFrame(train_pca, columns=mapper_pca.transformed_names_)\n",
    "\n",
    "#aplicar la técnica de redución de dimensionalidades\n",
    "pca_pipe = PCA(n_components=2, svd_solver=\"auto\", random_state=42)\n",
    "pca_pipe.fit(train_pca)\n",
    "results = pca_pipe.fit_transform(train_pca)\n",
    "\n",
    "#armar el dataframe con los datos obtenidos de la aplicación de PCA\n",
    "results_red=pd.DataFrame({'PCA1':results[:,0], 'PCA2':results[:,1], 'clase':train.is_home_run})\n",
    "\n",
    "\n",
    "\n",
    "#######################################################validation\n",
    "#pipe_pca1.fit(validation)\n",
    "#validation_pca=pipe_pca1.transform(validation)\n",
    "#validation_pca= pd.DataFrame(validation_pca, columns=mapper_pca.transformed_names_)\n",
    "\n",
    "#pca_pipe_v = PCA(n_components=2, svd_solver=\"auto\", random_state=42)\n",
    "#pca_pipe_v.fit(validation_pca)\n",
    "#results_v = pca_pipe_v.fit_transform(validation_pca)\n",
    "\n",
    "#armar el dataframe con los datos obtenidos de la aplicación de PCA\n",
    "#results_red_v=pd.DataFrame({'PCA1':results_v[:,0], 'PCA2':results_v[:,1], 'clase':validation.is_home_run})\n",
    "\n",
    "sns.scatterplot(x=\"PCA1\", y=\"PCA2\", hue='clase', data=results_red)\n",
    "\n",
    "sns.barplot(x=[\"PCA1\",\"PCA2\"], y=pca_pipe.explained_variance_ratio_)\n",
    "\n",
    "print(pca_pipe.explained_variance_ratio_)\n",
    "print(pca_pipe.explained_variance_ratio_.sum())\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7aaa4c0",
   "metadata": {},
   "source": [
    "Como se puede observar en lo anterior, podemos ver que aplicando dicha técnica pasamos a tener solamente dos componenetes que representan el 72% de la información total del dataset. Como consecuencia, con las componentes calculadas, se ahorraría tiempo y se podrían utilizar modelos como regresión logística con mejores resultados. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48c93d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pipel_red = Pipeline([\n",
    "#    ('classifier', LogisticRegression(random_state=42)),\n",
    "#])\n",
    "\n",
    "#results_red_e=results_red\n",
    "#results_red_e = results_red_e.drop(['clase'],axis=1)\n",
    "#print(results_red_v.head())\n",
    "\n",
    "#pipel_red.fit(results_red_e, results_red.clase)\n",
    "\n",
    "#y_pred = pipel_red.predict(results_red_v)\n",
    "#y_pred\n",
    "#evaluate_model_pca(pipel_red, title='LR Simple Imputer')\n",
    "\n",
    "#print(metrics.classification_report(results_red_v.clase, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2794e24",
   "metadata": {},
   "source": [
    "### Aplicación de técnica Umap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c82606",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "%matplotlib inline\n",
    "import umap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Armar y ejecución del procedimiento necesario de la técnica\n",
    "UMAP_Object=umap.UMAP(n_neighbors=5, min_dist=0.3, n_components=2)\n",
    "ComponentValues=UMAP_Object.fit_transform(train_pca)\n",
    " \n",
    "#Armar el dataframe con los datos obtenidos\n",
    "ReducedData=pd.DataFrame(data=ComponentValues, columns=['Comp1','Comp2'])\n",
    "print(ReducedData.head(10))\n",
    "\n",
    "#plt.scatter(x=\"Comp1\"[:,0], y=\"Comp2\"[:,1], data=ReducedData)\n",
    "#plt.title('UMAP embedding of random colours');\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
